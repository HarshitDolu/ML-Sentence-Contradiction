# -*- coding: utf-8 -*-
"""Contradiction_sentence.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15HrKRpYIlSOvN1zUtkIZ1e1g7vd-r3CU
"""

from google.colab import drive
from google.colab import files
#drive.mount('/content/drive')
import pandas as pd
import numpy as np
data=pd.read_csv("/content/Contradiction.csv")

data.head(5)

data.count()

data.drop('Unnamed: 0',axis=1,inplace=True)

data.head(2)

from sklearn import preprocessing
lencoder=preprocessing.LabelEncoder()
data['label']=lencoder.fit_transform(data['label'])

data.head(5)

data['label'].value_counts()

## 0------------->contradiction
## 1--------------->Entailment
## 2---------------->Neutral

################ step 1: feature extraction

####################Word count in each sentence

data['word_count1'] = data['sent1'].apply(lambda x: len(str(x).split(" ")))
data[['sent1','word_count1']].head()

data['word_count2'] = data['sent2'].apply(lambda x: len(str(x).split(" ")))
data[['sent2','word_count2']].head()

data[['sent1','sent2','word_count1','word_count2']].head(2)

####################Character count

data['char_count1'] =data['sent1'].str.len()
data[['sent1','char_count1']].head()

data['char_count2'] =data['sent2'].str.len()
data[['sent2','char_count2']].head()

####################### Average word length

def avg_word(sentence):
  words = sentence.split()
  return (sum(len(word) for word in words)/len(words))

data['avg_word1'] =data['sent1'].apply(lambda x: avg_word(x))
data[['sent1','avg_word1']].head()

def avg_word(sentence):
  words = sentence.split()
  return (sum(len(word) for word in words)/len(words))

data['avg_word2'] =data['sent2'].apply(lambda x: avg_word(x))
data[['sent2','avg_word2']].head()

######################## Stop words
#import nltk
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stop = stopwords.words('english')
data['stopwords_sen1'] =data['sent1'].apply(lambda x: len([x for x in x.split() if x in stop]))
data[['sent1','stopwords_sen1']].head()

stop = stopwords.words('english')
data['stopwords_sen2'] =data['sent2'].apply(lambda x: len([x for x in x.split() if x in stop]))
data[['sent2','stopwords_sen2']].head()

data['numerics_1'] =data['sent1'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))
data[['sent1','numerics_1']].head()

data['numerics_2'] =data['sent2'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))
data[['sent2','numerics_2']].head()

#################### converting to lower case
data['sent1'] =data['sent1'].apply(lambda x: " ".join(x.lower() for x in x.split()))
data['sent1'].head()

data['sent2'] =data['sent2'].apply(lambda x: " ".join(x.lower() for x in x.split()))
data['sent2'].head()

########################## Lemmatizing
nltk.download('wordnet')
from textblob import Word
data['sent1'] =data['sent1'].apply(lambda x: " ".join([Word(word).lemmatize() for word in x.split()]))
data['sent1'].head()

from textblob import Word
data['sent2'] =data['sent2'].apply(lambda x: " ".join([Word(word).lemmatize() for word in x.split()]))
data['sent2'].head()

data.head(1)

###################################### Storing bigram of sentences
from textblob import TextBlob
nltk.download('punkt')
lst1=[]
for i in range(data.sent1.count()):
  p=TextBlob(sen1[i]).ngrams(2)
  lst1.append(p)

lst1[0][0]

lst2=[]
for i in range(data.sent2.count()):
  p=TextBlob(data['sent2'][i]).ngrams(2)
  lst2.append(p)

######################## implementing tf-idf sent1
from sklearn.feature_extraction.text import TfidfVectorizer
vecto=TfidfVectorizer()
ltf=[]
for i in range(4500):
  res1=vecto.fit_transform([sen1[i]])
  ltf.append(res1.toarray())

data.head(1)

#print(res1.toarray())

res2=vecto.fit_transform([sen1[0]])
res=vecto.fit_transform([sen2[0]])
res2.toarray()
res=res.toarray()

############################################# Jacard similarity##############################################
def get_jaccard_sim(str1, str2): 
    a = set(str1.split()) 
    b = set(str2.split())
    c = a.intersection(b)
    return float(len(c)) / (len(a) + len(b) - len(c))
for i in range(data.sent1.count()):
    data['jacard']=get_jaccard_sim(data.sent1[i], data.sent2[i])

data.jacard.value_counts()

A=data.sent1[0]

A

data.head(1)

from collections import Counter
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
pt=[]
for i in range(4500):
  ls1=sen1[i].split(" ")
  ls2=sen2[i].split(" ")
  bow={}
  ls=ls1+ls2
  ls=set(ls)  ############## unique words
  ls=list(ls)
      #ls1=set(ls1)
      #ls2=set(ls2)
  for i in ls:
      if i in ls1:
        if i not in bow.keys() :
            bow[i]=1
        else:
            bow[i]=bow[i]+1
      else:
          bow[i]=0
      #print(bow)
      lbow =[v for v in bow.values()]
  #print(lbow)
  p=np.asarray(lbow)
  pt.append(p)

  #data['sent1'][i]=p
  #p=lbow
  #data.sent1=
  #print(bow)
    #bow={}
    #print(lbow)
    #lk=lk.append(lbow)
#print(ls)
#vectorizer = CountVectorizer()
#res1=vectorizer.fit_transform(ls,ls1)
#res2=vectorizer.fit_transform(ls,ls2)
#lc1=res1.toarray()
#lc2=res2.toarray()
import scipy.spatial.distance as d
#c=1-d.cosine(lc1,lc2)
#lc1=lc1.reshape(1,-1)
#lc2=lc2.reshape(1,-1)

#c=1-d.cosine(lc1,lc2)
#print(lc1)
#print(cosine_similarity(lc1,lc2))
#print(c)

sen1=[]
for i in range(4500):
  sen1.append(data.sent1[i])

sen1[0]

data['sent1']=pt

data.head(1)

data.head(2)

sen2=[]
for i in range(4500):
  sen2.append(data.sent2[i])

from collections import Counter
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
pt=[]
for i in range(4500):
  ls1=sen1[i].split(" ")
  ls2=sen2[i].split(" ")
  bow={}
  ls=ls1+ls2
  ls=set(ls)  ############## unique words
  ls=list(ls)
      #ls1=set(ls1)
      #ls2=set(ls2)
  for i in ls:
      if i in ls2:
        if i not in bow.keys() :
            bow[i]=1
        else:
            bow[i]=bow[i]+1
      else:
          bow[i]=0
      #print(bow)
      lbow =[v for v in bow.values()]
  #print(lbow)
  p=np.asarray(lbow)
  pt.append(p)

import scipy.spatial.distance as d
#c=1-d.cosine(lc1,lc2)
#lc1=lc1.reshape(1,-1)
#lc2=lc2.reshape(1,-1)
#c=1-d.cosine(lc1,lc2)
#print(lc1)
#print(cosine_similarity(lc1,lc2))
#print(c)

data['sent2']=pt

#data.sent1[0].shape
data.sent2[0].shape

lst=[]
for i in range(4500):
    l1=data.sent1[i].reshape(1,-1)
    l2=data.sent2[i].reshape(1,-1)
    lst.append(cosine_similarity(l1,l2))

data['cosin_sim']=lst

data.head(2)

sen1[0]

import sklearn
sc=sklearn.metrics.pairwise.cosine_similarity(data.sent1[0],data.sent2[0])

lj=[]
 def get_jaccard_sim(str1, str2): 
    a = set(str1.split()) 
    b = set(str2.split())
    c = a.intersection(b)
    return float(len(c)) / (len(a) + len(b) - len(c))
for i in range(data.sent1.count()):
    lj.append(get_jaccard_sim(sen1[i],sen2[i]))

data['jacard']=lj

data.head(2)

sen1[0]

s=set(sen1[0].split(" "))

from nltk.corpus import stopwords
import nltk
nltk.download('averaged_perceptron_tagger')
stop_words = set(stopwords.words('english'))

ln=[]
for i in range(4500):
  k=0
  s=set(sen1[i].split(" "))
  wl=[w for w in s if not w in stop_words]
  tag=nltk.pos_tag(wl)
  #print(tag)
  for j in range(len(tag)):
    if tag[j][1]=='NN':
      k=k+1
  ln.append(k)

data['sent1_nouns']=ln

ln=[]
for i in range(4500):
  k=0
  s=set(sen2[i].split(" "))
  wl=[w for w in s if not w in stop_words]
  tag=nltk.pos_tag(wl)
  for j in range(len(tag)):
    if tag[j][1]=='NN':
      k=k+1
  ln.append(k)

data['sent2_nouns']=ln

data.head(2)

from textblob import TextBlob
lp=[]
for i in range(4500):
  sentt=TextBlob(sen1[i])
  pol=sentt.sentiment.polarity
  lp.append(pol)

data['sentiment_sent1']=lp

from textblob import TextBlob
lp=[]
for i in range(4500):
  sentt=TextBlob(sen2[i])
  pol=sentt.sentiment.polarity
  lp.append(pol)

data['sentiment_sent2']=lp

data.head(2)

ln=[]
for i in range(4500):
  k=0
  s=set(sen1[i].split(" "))
  wl=[w for w in s if not w in stop_words]
  tag=nltk.pos_tag(wl)
  #print(tag)
  for j in range(len(tag)):
    if tag[j][1]=='VBG'or tag[j][1]=='VBN'or tag[j][1]=='VBP' or tag[j][1]=='VBZ':
      k=k+1
  ln.append(k)

data['Verb_sent1']=ln

ln=[]
for i in range(4500):
  k=0
  s=set(sen2[i].split(" "))
  wl=[w for w in s if not w in stop_words]
  tag=nltk.pos_tag(wl)
  #print(tag)
  for j in range(len(tag)):
    if tag[j][1]=='VBG'or tag[j][1]=='VBN'or tag[j][1]=='VBP' or tag[j][1]=='VBZ':
      k=k+1
  ln.append(k)

data['verb_sent2']=ln

data.head(2)

from nltk.corpus import movie_reviews
nltk.download('movie_reviews')
neg1=[]
doc=movie_reviews.words('neg/cv000_29416.txt')
for i in range(4500):
  s=sen1[i].split(" ")
  v=0
  for j in s:
    if j in doc:
      v=v+1
  neg1.append(v)

data['neg_sent1_review']=neg1

neg1

neg2=[]
doc=movie_reviews.words('neg/cv000_29416.txt')
for i in range(4500):
  s=sen2[i].split(" ")
  v=0
  for j in s:
    if j in doc:
      v=v+1
  neg2.append(v)

data['neg_sent2_review']=neg2

data.head(2)

from sklearn.feature_extraction.text import TfidfVectorizer
vecto=TfidfVectorizer()
ltf1=[]
for i in range(4500):
  res1=vecto.fit_transform([sen2[i]])
  ltf1.append(res1.toarray())

data['tf-idf_sent1']=ltf
data['tf-idf_sent2']=ltf1

data.head(1)

data.count()

from textblob import TextBlob
nltk.download('punkt')
lst2=[]
for i in range(data.sent1.count()):
  p=TextBlob(sen2[i]).ngrams(2)
  lst2.append(p)

lst1[0]

kn1=[]
for i in range(len(lst1)):
  p=list(lst1)
  kn1.append(list(p[0][0]))

kn2=[]
for i in range(len(lst2)):
  p=list(lst2)
  kn2.append(list(p[0][0]))

bowsen1=data['sent1']
bowsen2=data['sent2']

len(kn1[0])

y=data['label']

y
data.drop(['label'],axis=1,inplace=True)

data.drop(['sent1'],axis=1,inplace=True)
data.drop(['sent2'],axis=1,inplace=True)

data.head(1)

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
x=data
import sklearn.model_selection as model_selection
xtrain,xtest,ytrain,ytest=model_selection.train_test_split(x,y,test_size=0.2,random_state=4)
krange=range(1,10)
scores={}
s_list=[]
for k in krange:
  clf=KNeighborsClassifier(n_neighbors=1)
  clf.fit(xtrain,ytrain)
  ypred=clf.predict(xtest)
  scores[k]=accuracy_score(ytest,ypred)
  s_list.append(accuracy_score(ytest,ypred))

data.head(1)

lc=[]
for i in range(4500):
 lc.append(data.cosin_sim[i][0][0])

data['cosin_sim']=lc

data.drop(['tf-idf_sent1','tf-idf_sent2'],axis=1,inplace=True)

data.to_csv('sentence_contradiction.csv')

##############################Knn algo#########################################
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
x=data
import sklearn.model_selection as model_selection
xtrain,xtest,ytrain,ytest=model_selection.train_test_split(x,y,test_size=0.2,random_state=4)

xtrain.head(1)

krange=range(1,10)
scores={}
s_list=[]
for k in krange:
  clf=KNeighborsClassifier(n_neighbors=7)
  clf.fit(xtrain,ytrain)
  ypred=clf.predict(xtest)
  scores[k]=accuracy_score(ytest,ypred)
  s_list.append(accuracy_score(ytest,ypred))

import matplotlib.pyplot as plt
plt.plot(krange,s_list)
plt.xlabel('value of k for knn')
plt.ylabel('Testing accuracy')

from sklearn.metrics import classification_report
print(classification_report(ytest,ypred))

clf=KNeighborsClassifier(n_neighbors=9)
clf.fit(xtrain,ytrain)
ypred=clf.predict(xtest)
scores=accuracy_score(ytest,ypred)
print(accuracy_score(ytest,ypred))

######################### Logistic ###############################
from sklearn.linear_model import LogisticRegression

from sklearn.metrics import accuracy_score
clf=LogisticRegression()
clf.fit(xtrain,ytrain)
ypred=clf.predict(xtest)
scores=accuracy_score(ytest,ypred)
print(accuracy_score(ytest,ypred))

from sklearn.metrics import classification_report
print(classification_report(ytest,ypred))

from sklearn.naive_bayes import BernoulliNB
from sklearn.metrics import accuracy_score
x=data
import sklearn.model_selection as model_selection
from sklearn.metrics import accuracy_score
clf=BernoulliNB(binarize=0.0)
clf.fit(xtrain,ytrain)
ypred=clf.predict(xtest)
scores=accuracy_score(ytest,ypred)
print(accuracy_score(ytest,ypred))

from sklearn.metrics import classification_report
print(classification_report(ytest,ypred))

